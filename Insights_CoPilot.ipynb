{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup & Installation (Refined)\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Checks if required packages are installed and installs them if missing.\"\"\"\n",
    "    packages = ['google-generativeai', 'pandas', 'python-dotenv', 'openpyxl']\n",
    "    print(\"Checking required packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            # A quick way to check if a package is installed is to try importing it\n",
    "            __import__(package.replace('-', '_'))\n",
    "            print(f\"âœ… {package} is already installed.\")\n",
    "        except ImportError:\n",
    "            print(f\"ðŸ“¦ Package '{package}' not found. Installing...\")\n",
    "            # Use subprocess to install the package quietly\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "            print(f\"âœ… {package} installed successfully.\")\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c73f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Configuration and Class Definitions (New & Refined)\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import io\n",
    "\n",
    "# --- 1. Centralized Configuration ---\n",
    "class Config:\n",
    "    \"\"\"Holds all configuration settings for the co-pilot.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.api_key: Optional[str] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "        self.model_name: str = \"gemini-2.5-flash\"\n",
    "        self.data_file_path: Path = Path(\"CONSOLIDATED_OUTPUT_DATA.csv\")\n",
    "        self.db_summary_path: Path = Path(\"db_summary.json\")\n",
    "        self.kpi_mapping_path: Path = Path(\"context_kpi_mapping.json\")\n",
    "        self.queries_file_path: Path = Path(\"User Queries.xlsx\")\n",
    "        self.output_dir: Path = Path(\"copilot_runs\")\n",
    "        self.output_dir.mkdir(exist_ok=True)  # Ensure the main output directory exists\n",
    "        self.max_retries: int = 2 # <<<< NEW: Maximum number of retries on code execution failure\n",
    "\n",
    "    def validate(self):\n",
    "        \"\"\"Validates the configuration, especially the API key.\"\"\"\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\n",
    "                'âŒ API Key not found! Please create a .env file with GOOGLE_API_KEY=\"your_key\".'\n",
    "            )\n",
    "        genai.configure(api_key=self.api_key)\n",
    "        print(\"âœ… API Key configured successfully!\")\n",
    "\n",
    "\n",
    "# --- 2. Data Handling Class ---\n",
    "class DataLoader:\n",
    "    \"\"\"Handles loading all necessary data and context files.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self.df: Optional[pd.DataFrame] = None\n",
    "        self.db_summary: Optional[Dict] = None\n",
    "        self.kpi_mapping: Optional[Dict] = None\n",
    "\n",
    "    def load_all(self) -> bool:\n",
    "        \"\"\"Loads all data sources and returns True if successful.\"\"\"\n",
    "        try:\n",
    "            print(f\"â³ Loading main data from '{self.config.data_file_path}'...\")\n",
    "            self.df = pd.read_csv(self.config.data_file_path)\n",
    "            print(\n",
    "                f\"âœ… Successfully loaded main data ({self.df.shape[0]} rows, {self.df.shape[1]} columns).\"\n",
    "            )\n",
    "\n",
    "            print(f\"â³ Loading DB summary from '{self.config.db_summary_path}'...\")\n",
    "            with open(self.config.db_summary_path, \"r\") as f:\n",
    "                self.db_summary = json.load(f)\n",
    "            print(\"âœ… Successfully loaded DB summary.\")\n",
    "\n",
    "            print(f\"â³ Loading KPI mapping from '{self.config.kpi_mapping_path}'...\")\n",
    "            with open(self.config.kpi_mapping_path, \"r\") as f:\n",
    "                self.kpi_mapping = json.load(f)\n",
    "            print(\"âœ… Successfully loaded KPI mapping.\")\n",
    "            return True\n",
    "\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"âŒ ERROR: A required file was not found: {e.filename}\")\n",
    "            return False\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ An error occurred during data loading: {e}\")\n",
    "            return False\n",
    "\n",
    "\n",
    "# --- 3. Gemini Agent Class ---\n",
    "class InsightsAgent:\n",
    "    \"\"\"Encapsulates all interactions with the Gemini API.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        self.model = genai.GenerativeModel(config.model_name)\n",
    "        self.config = config\n",
    "\n",
    "    def generate_analysis_code(\n",
    "        self, user_query: str, db_summary: str, kpi_mapping: str\n",
    "    ) -> str:\n",
    "        \"\"\"Generates Python code using a refined, detailed prompt.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "            You are a **world-class market research data analyst and Python developer**.  \n",
    "            Your task is to generate a **fully self-contained Python script** that answers the user's analytical question with **high accuracy** using **pandas** and **numpy**.  \n",
    "\n",
    "            You must strictly follow the **provided data context** and the **analytical playbook**.\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### 1. User Question\n",
    "            {user_query}\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### 2. Data Source\n",
    "            - The data is in a CSV file located at: {self.config.data_file_path.resolve()}\n",
    "            - You must load the data **directly from this absolute path** (do not modify it).\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### 3. Database Summary (`db_summary.json`)\n",
    "            This JSON describes the table columns. Use it to understand dimensions and measures.  \n",
    "            - Demographics are in the `\"Datacut\"` column.  \n",
    "            - Time periods are in the `\"Time_Period\"` column.  \n",
    "            - Numeric values are in the `\"value\"` column.  \n",
    "\n",
    "            ```json\n",
    "            {db_summary}\n",
    "            ```\n",
    "            ---\n",
    "\n",
    "            ### 4. Contextâ€“KPI Mapping\n",
    "            This JSON defines how to map Context and KPI/Brand correctly.  \n",
    "            - First, filter by the relevant \"Context\" based on the user's question.  \n",
    "            - Then, filter by \"KPI\" or \"Brand\".  \n",
    "            \n",
    "            ```json\n",
    "            {kpi_mapping}\n",
    "            ```\n",
    "\n",
    "            ---\n",
    "\n",
    "            ### 5. Analytical Playbook (STRICT RULES)\n",
    "\n",
    "            **Time Handling** - `Time_Period` is formatted as `\"H1'25\"` (first half 2025).  \n",
    "            - Always sort chronologically (not alphabetically).  \n",
    "\n",
    "            **Demographics Logic** - If the user does not specify a demographic, default to `\"Total\"`.  \n",
    "\n",
    "            **All Brands/KPIs**\n",
    "            - If the user does not specify a brand or KPI, then instead of not applying any filter on brands/kpis, you must pass a filter to include **all** brands/KPIs present in the (`db_summary.json`).\n",
    "\n",
    "            **Exclude Non-Analytical Metrics** - ALWAYS EXCLUDE \"Unweighted Base\", \"Sample Size\", and \"Base\" from calculations, averages, rankings, and outputs, unless explicitly requested by the user to include it.\n",
    "\n",
    "            **Change Over Time** To calculate change:  \n",
    "            a. Filter data.  \n",
    "            b. Create a pivot with `Time_Period` as columns.  \n",
    "            c. Subtract the older periodâ€™s value from the newer periodâ€™s value.  \n",
    "\n",
    "            **Ranking Logic** For \"Top/Bottom N\" requests, use:  \n",
    "            ```python\n",
    "            df.sort_values(by=\"value\", ascending=False).head(N)\n",
    "            ```\n",
    "            \n",
    "            **Detail of Final Dataframe**\n",
    "            - The final dataframe must explanatory. So instead of just the final number, include relevant summarized breakdowns used to arrive at the final answer.\n",
    "            - For example:\n",
    "                - If you are asked to find the top 5 brands, include the brand names and their values in the final output.\n",
    "                - If you are asked for change over time, include both time periods and the calculated change.\n",
    "\n",
    "            **Output Column** - Always use `\"value\"` column for calculations (these are given as absolute numbers, but represent percentage share except Base and Index Metrics).  \n",
    "\n",
    "            ---\n",
    "\n",
    "            ### 6. Final Script Requirements\n",
    "            - Must be a complete, runnable Python script.  \n",
    "            - Must use **pandas** and **numpy** only (no external libraries).  \n",
    "            - Must produce a single pandas DataFrame named `result_df`.  \n",
    "            - The final output must be a SINGLE pandas DataFrame printed to the console print(result_df.to_string())\n",
    "            - Ensure no truncation, partial tables, or additional debug printouts.  \n",
    "\n",
    "            ---\n",
    "\n",
    "            ### OUTPUT FORMAT\n",
    "            Your response should contain **only the Python script**.  \n",
    "            Do not include explanations or extra commentary.\n",
    "            \"\"\"\n",
    "        response = self.model.generate_content(prompt)\n",
    "        code = response.text.strip()\n",
    "        if \"python\" in code:\n",
    "            code = code.split(\"python\")[1].split(\"```\")[0]\n",
    "        return code.strip()\n",
    "\n",
    "    # <<<< NEW METHOD: To handle code regeneration on error >>>>\n",
    "    def regenerate_code_on_error(self, failed_code: str, error_message: str, user_query: str) -> str:\n",
    "        \"\"\"Takes failed code and an error message, then asks the model to fix it.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "            You are an expert Python debugging assistant.\n",
    "            The following Python script, which was written to answer the question '{user_query}', failed during execution.\n",
    "\n",
    "            ### FAILED SCRIPT\n",
    "            ```python\n",
    "            {failed_code}\n",
    "            ```\n",
    "\n",
    "            ### ERROR MESSAGE\n",
    "            ```\n",
    "            {error_message}\n",
    "            ```\n",
    "\n",
    "            ### INSTRUCTIONS\n",
    "            1.  **Analyze** the error message and the original code.\n",
    "            2.  **Correct the script** to resolve the error while still fulfilling the original user query.\n",
    "            3.  **Adhere strictly** to all the original script requirements (use pandas/numpy, produce `result_df`, print with `to_string()`).\n",
    "            4.  Your output must be **only the complete, corrected, runnable Python script**. Do not provide explanations, apologies, or any text other than the code itself.\n",
    "            \"\"\"\n",
    "        print(\"ðŸ¤– Asking the agent to fix the code based on the error...\")\n",
    "        response = self.model.generate_content(prompt)\n",
    "        code = response.text.strip()\n",
    "        if \"python\" in code:\n",
    "            code = code.split(\"python\")[1].split(\"```\")[0]\n",
    "        return code.strip()\n",
    "\n",
    "\n",
    "    def generate_insight_summary(self, data_table_string: str, user_query: str) -> str:\n",
    "        \"\"\"Generates a direct answer to the userâ€™s question with supporting data evidence.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        You are a senior market research analyst. Your task is to provide a clear, fact-based answer to the userâ€™s question.\n",
    "\n",
    "        ### Original User Question:\n",
    "        \"{user_query}\"\n",
    "\n",
    "        ### Data Analysis Results:\n",
    "        {data_table_string}\n",
    "\n",
    "        ### Instructions:\n",
    "        - **Values from Analysis:** Use only the data provided in the analysis results above. Do not assume or invent any data.\n",
    "        - **Type of Values:** The values represent percentage shares or Index scores and not aboslute counts or numbers, except for Bases. So if a number is 25, it means 25% share or an Index of 25 not 25 counts.\n",
    "        - **Direct Answer:** Start with a precise response to the userâ€™s question, framed as a clear statement.  \n",
    "        - **Supporting Evidence (1-2 bullets):** Use the most relevant numbers, comparisons, or trends from the data to prove the answer.  \n",
    "        - **Clarity:** Be concise, avoid fluff, and focus only on insights supported by the data provided.  \n",
    "        - **Format:** Use Markdown for readability (headings, bold, bullet points where relevant).  \n",
    "        \"\"\"\n",
    "        response = self.model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "\n",
    "\n",
    "print(\"âœ… All classes (Config, DataLoader, InsightsAgent) are defined and ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b48983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Initialization\n",
    "try:\n",
    "    config = Config()\n",
    "    config.validate()\n",
    "\n",
    "    data_loader = DataLoader(config)\n",
    "    data_loaded_successfully = data_loader.load_all()\n",
    "\n",
    "    agent = InsightsAgent(config)\n",
    "    print(\"\\nâœ… Co-pilot initialized successfully!\")\n",
    "except (ValueError, FileNotFoundError) as e:\n",
    "    print(f\"\\nâŒ Initialization failed: {e}\")\n",
    "    data_loaded_successfully = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6b488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Main Orchestration (Refined with Classes and Retry Logic)\n",
    "\n",
    "# --- USER CONTROL PANEL ---\n",
    "START_ID = 1\n",
    "END_ID = 16\n",
    "# --------------------------\n",
    "\n",
    "def run_batch_processing(start_id: int, end_id: int):\n",
    "    \"\"\"Main function to run the batch processing of user queries.\"\"\"\n",
    "    if not data_loaded_successfully:\n",
    "        print(\"\\nâŒ Cannot run batch processing because data failed to load.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        queries_df = pd.read_excel(config.queries_file_path, sheet_name='Sheet 1')\n",
    "        selected_queries = queries_df[(queries_df['ID'] >= start_id) & (queries_df['ID'] <= end_id)]\n",
    "        print(f\"âœ… Loaded Excel file. Will process {len(selected_queries)} questions from ID {start_id} to {end_id}.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"âŒ ERROR: '{config.queries_file_path}' not found. Please ensure the file exists.\")\n",
    "        return\n",
    "\n",
    "    batch_results_list = []\n",
    "    \n",
    "    # Prepare context strings once\n",
    "    db_summary_str = json.dumps(data_loader.db_summary, indent=2)\n",
    "    kpi_mapping_str = json.dumps(data_loader.kpi_mapping, indent=2)\n",
    "\n",
    "    for _, row in selected_queries.iterrows():\n",
    "        query_id, user_query = row['ID'], row['Question']\n",
    "        print(\"\\n\" + \"=\"*80 + f\"\\nâ–¶ï¸ Processing Query ID: {query_id} | Query: '{user_query}'\\n\" + \"=\"*80)\n",
    "\n",
    "        run_folder_name = f\"run_ID_{query_id}_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "        output_dir = config.output_dir / run_folder_name\n",
    "        output_dir.mkdir()\n",
    "        print(f\"âœ… Outputs will be saved in: '{output_dir}'\")\n",
    "\n",
    "        data_output, final_summary = \"Error during processing.\", \"Not generated.\"\n",
    "        generated_code = \"\"\n",
    "        last_error_message = \"\"\n",
    "        code_executed_successfully = False\n",
    "\n",
    "        # <<<< NEW: Retry loop >>>>\n",
    "        for attempt in range(config.max_retries + 1):\n",
    "            print(f\"\\n--- Attempt {attempt + 1} of {config.max_retries + 1} ---\")\n",
    "            \n",
    "            try:\n",
    "                # Step 1: Generate or Regenerate Code\n",
    "                if attempt == 0:\n",
    "                    print(\"Step 1: Calling Insights Agent to generate initial code...\")\n",
    "                    generated_code = agent.generate_analysis_code(user_query, db_summary_str, kpi_mapping_str)\n",
    "                else:\n",
    "                    print(\"Step 1: Calling Insights Agent to regenerate code based on previous error...\")\n",
    "                    generated_code = agent.regenerate_code_on_error(generated_code, last_error_message, user_query)\n",
    "                \n",
    "                script_path = output_dir / f'generated_script_attempt_{attempt+1}.py'\n",
    "                script_path.write_text(generated_code)\n",
    "                print(f\"âœ… Code generated for attempt {attempt+1}.\")\n",
    "\n",
    "                # Step 2: Execute Code\n",
    "                print(\"Step 2: Executing generated script...\")\n",
    "                result = subprocess.run([sys.executable, str(script_path)], capture_output=True, text=True, check=True, timeout=60)\n",
    "                data_output = result.stdout\n",
    "                (output_dir / 'data_output.txt').write_text(data_output)\n",
    "                print(\"âœ… Script executed successfully.\")\n",
    "                code_executed_successfully = True\n",
    "                break # <<<< Exit the retry loop on success\n",
    "\n",
    "            except subprocess.TimeoutExpired:\n",
    "                last_error_message = \"SCRIPT EXECUTION FAILED: Timeout after 60 seconds.\"\n",
    "                print(f\"âŒ {last_error_message}\")\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                last_error_message = f\"SCRIPT EXECUTION FAILED:\\n\\n{e.stderr}\"\n",
    "                print(f\"âŒ {last_error_message}\")\n",
    "            except Exception as e:\n",
    "                last_error_message = f\"An unexpected error occurred during processing: {e}\"\n",
    "                print(f\"âŒ {last_error_message}\")\n",
    "\n",
    "        # After the loop, check for success\n",
    "        if code_executed_successfully:\n",
    "            # Step 3: Generate Insights\n",
    "            if data_output and data_output.strip():\n",
    "                print(\"\\nStep 3: Generating final summary...\")\n",
    "                final_summary = agent.generate_insight_summary(data_output, user_query)\n",
    "                (output_dir / 'final_summary.md').write_text(final_summary)\n",
    "                print(\"âœ… Summary generated successfully.\")\n",
    "            else:\n",
    "                final_summary = \"No insights generated (empty data output).\"\n",
    "        else:\n",
    "            # If all retries failed\n",
    "            print(f\"\\nâŒ All {config.max_retries + 1} attempts failed for Query ID {query_id}.\")\n",
    "            data_output = last_error_message # Store the final error\n",
    "            final_summary = \"Failed to generate insights after multiple attempts.\"\n",
    "\n",
    "        batch_results_list.append({'ID': query_id, 'Question': user_query, 'Data Output': data_output, 'Insight Summary': final_summary})\n",
    "\n",
    "    # After loop, create the consolidated report\n",
    "    if batch_results_list:\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\nâœ… Batch processing complete. Creating consolidated summary report...\")\n",
    "        summary_df = pd.DataFrame(batch_results_list)\n",
    "        summary_path = config.output_dir / f\"Batch_Run_Summary_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.xlsx\"\n",
    "        summary_df.to_excel(summary_path, index=False)\n",
    "        print(f\"ðŸŽ‰ðŸŽ‰ðŸŽ‰ Successfully created summary report: '{summary_path}' ðŸŽ‰ðŸŽ‰ðŸŽ‰\")\n",
    "\n",
    "# --- RUN THE BATCH PROCESS ---\n",
    "run_batch_processing(START_ID, END_ID)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
